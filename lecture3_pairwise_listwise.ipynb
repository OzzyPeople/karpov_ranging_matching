{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинарная кросс энтропия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "$$C_{ij}=C(o_{ij})=-\\bar{P_{ij}}log(P_{ij})-(1-\\bar{P_{ij}})log(1-P_{ij})$$\n",
    "\n",
    "реальные_отметки $$\\bar{P_{ij}}$$ \n",
    "предсказания вероятности i стремится к 1, а j меньше i $${P_{ij}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  предсказание модели\n",
    "\n",
    "f(x_i) - логиты или скоры, предсказание i объктам по i признакам \n",
    "\n",
    "f(x_i) - логиты или скоры, предсказание j объктам по j признакам \n",
    "\n",
    "%%latex\n",
    "$$o_{ij}=f(x_i)-f(x_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция активации - сигмоид\n",
    "\n",
    "%%latex\n",
    "$$P_{ij}=\\frac{e^{o_{ij}}}{1+e^{o_{ij}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "если o > 1 то вероятность того, что i выше в ранжировании, чем j-й стремится к 1 \n",
    "\n",
    "если o < 0, то стремится к 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "$$\\text{out}_{i} = \\frac{1}{1 + e^{-\\text{input}_{i}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankNet(torch.nn.Module):\n",
    "    # num_input_features - количество признаком или размер вектора \n",
    "    # hidden_dim - количество скрытых слоев \n",
    "    def __init__(self, num_input_features, hidden_dim=10):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = torch.nn.Sequential(\n",
    "            #входной слой - перевод размерности входных признаков в размерность скрытого слоя\n",
    "            torch.nn.Linear(num_input_features, self.hidden_dim),  \n",
    "            torch.nn.ReLU(), #функция нелинейности для апроксимации любой нелинейной функции или величины \n",
    "            torch.nn.Linear(self.hidden_dim, 1), #слой из скрытой размерности предсказывает 1 скаляр - наша оценка релевантности \n",
    "        )\n",
    "        #функция активации - сигмоид \n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_1, input_2):\n",
    "        logits_1 = self.predict(input_1) # для i документа \n",
    "        logits_2 = self.predict(input_2)  # для j документа \n",
    "        \n",
    "        logits_diff = logits_1 - logits_2\n",
    "        out = self.out_activation(logits_diff)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        logits = self.model(inp) #вызываем модель и переводим вектор в скаляр - оценку релевантности \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranknet_model = RankNet(num_input_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7165, 0.7661, 0.7220, 0.8430, 0.8012, 0.8874, 0.7183, 0.5816, 0.4342,\n",
       "         0.8021],\n",
       "        [0.1840, 0.9046, 0.8894, 0.7193, 0.2272, 0.5759, 0.4223, 0.8312, 0.7544,\n",
       "         0.7523],\n",
       "        [0.4624, 0.0366, 0.5725, 0.3309, 0.8656, 0.6027, 0.0769, 0.3857, 0.2628,\n",
       "         0.8071],\n",
       "        [0.1350, 0.3127, 0.6222, 0.4139, 0.5463, 0.7127, 0.0887, 0.6781, 0.4118,\n",
       "         0.5569]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_1, inp_2 = torch.rand(4, 10), torch.rand(4, 10)\n",
    "# 4 - количество документов, которые мы рассматриваем \n",
    "# 10 - количество признаков, которые будут подаваться на вход нейронной сети \n",
    "# batch_size x input_dim\n",
    "inp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4901],\n",
       "        [0.4709],\n",
       "        [0.5091],\n",
       "        [0.5143]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = ranknet_model(inp_1, inp_2)\n",
    "#вероятность того, что каждый объект из inp_1 более релевантен, чем соответсвующий объект из input 2 \n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#возьмем первый слой модели \n",
    "first_linear_layer = ranknet_model.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_linear_layer.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss() #добавим бинарную кросс энтропию как нашу функцию потерь \n",
    "loss = criterion(preds, torch.ones_like(preds)) #добавим наши предсказания и предположим, что все i документы более релевантны, чем j документы \n",
    "loss.backward() #расчет градиента методом обратного распостранения ошибки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7016, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-1.1152e-02, -7.1024e-04,  5.8067e-03,  5.0189e-03, -2.8768e-03,\n",
       "          3.5013e-02, -2.1032e-02,  1.1283e-02,  2.1437e-02,  2.4344e-02],\n",
       "        [-3.4220e-02, -2.1653e-02, -2.5081e-02, -3.6982e-03, -2.3296e-02,\n",
       "          8.4429e-03, -2.3206e-02,  2.4566e-03,  8.0513e-03, -7.0617e-03],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-3.1268e-04, -5.0490e-04, -5.1632e-04, -4.9343e-04, -6.5922e-04,\n",
       "         -3.8938e-04, -4.4890e-04, -3.0976e-04, -2.7555e-04, -3.2626e-04],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-8.7109e-04, -2.4860e-03, -1.0810e-03, -1.7250e-03, -1.5743e-03,\n",
       "         -9.3762e-05, -2.4604e-03, -4.6234e-05, -5.1353e-04,  3.8061e-04]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_linear_layer.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranknet_model.zero_grad() #обнуляем веса, это необходимо делать перед каждым прогоном для обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ListNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "from src.utils import ndcg, num_swapped_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListNet(torch.nn.Module):\n",
    "    def __init__(self, num_input_features, hidden_dim=10):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_input_features, self.hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input_1):\n",
    "        logits = self.model(input_1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logits\n",
    "\n",
    "In deep learning, the term logits layer is popularly used for the last neuron layer of neural network for classification task which produces raw prediction values as real numbers ranging from [-infinity, +infinity ].\n",
    "\n",
    "— Wikipedia\n",
    "Logits are the raw scores output by the last layer of a neural network. Before activation takes place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс энтропия\n",
    "\n",
    "подходит для нескольких классов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "$$CE = -\\sum ^{N}_{j=1} (P_y^i(j) * log(P_z^i(j)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "$$\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax turn logits (numeric output of the last linear layer of a multi-class classification neural network) into probabilities by take the exponents of each output and then normalize each number by the sum of those exponents so the entire output vector adds up to one — all probabilities should add up to one.\n",
    "\n",
    "\n",
    "Softmax function turns logits  into probabilities and the probabilities sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6590011388859679, 0.2424329707047139, 0.09856589040931818]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = [2.0, 1.0, 0.1]\n",
    "exps = [np.exp(logit) for logit in logits]\n",
    "sum_of_exps = sum(exps)\n",
    "softmax = [j/sum_of_exps for j in exps]\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listnet_ce_loss(y_i, z_i): \n",
    "    \"\"\"\n",
    "    y_i: (n_i, 1) GT  - разметка из данных \n",
    "    z_i: (n_i, 1) preds - оценки модели \n",
    "    \"\"\"\n",
    "\n",
    "    P_y_i = torch.softmax(y_i, dim=0)\n",
    "    P_z_i = torch.softmax(z_i, dim=0)\n",
    "    return -torch.sum(P_y_i * torch.log(P_z_i))\n",
    "\n",
    "def listnet_kl_loss(y_i, z_i): #дивергенция кульбака-лейблера\n",
    "    \"\"\"\n",
    "    y_i: (n_i, 1) GT\n",
    "    z_i: (n_i, 1) preds\n",
    "    \"\"\"\n",
    "    P_y_i = torch.softmax(y_i, dim=0)\n",
    "    P_z_i = torch.softmax(z_i, dim=0)\n",
    "    return -torch.sum(P_y_i * torch.log(P_z_i/P_y_i))\n",
    "\n",
    "\n",
    "def make_dataset(N_train, N_valid, vector_dim):\n",
    "    fake_weights = torch.randn(vector_dim, 1)\n",
    "\n",
    "    X_train = torch.randn(N_train, vector_dim)\n",
    "    X_valid = torch.randn(N_valid, vector_dim)\n",
    "\n",
    "    ys_train_score = torch.mm(X_train, fake_weights)\n",
    "    ys_train_score += torch.randn_like(ys_train_score) #добавляем шум \n",
    "\n",
    "    ys_valid_score = torch.mm(X_valid, fake_weights) #создадим таргеты и пошумим на них \n",
    "    ys_valid_score += torch.randn_like(ys_valid_score)\n",
    "\n",
    "#     bins = [-1, 1]  # 3 relevances\n",
    "    bins = [-1, 0, 1, 2]  # 5 relevances\n",
    "    ys_train_rel = torch.Tensor(\n",
    "        np.digitize(ys_train_score.clone().detach().numpy(), bins=bins)\n",
    "    )\n",
    "    ys_valid_rel = torch.Tensor(\n",
    "        np.digitize(ys_valid_score.clone().detach().numpy(), bins=bins)\n",
    "    )\n",
    "\n",
    "    return X_train, X_valid, ys_train_rel, ys_valid_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 1000\n",
    "N_valid = 500\n",
    "\n",
    "vector_dim = 100\n",
    "epochs = 2\n",
    "\n",
    "batch_size = 16 #будет наши шагом градиентного спуска  \n",
    "\n",
    "X_train, X_valid, ys_train, ys_valid = make_dataset(N_train, N_valid, vector_dim)\n",
    "\n",
    "net = ListNet(num_input_features=vector_dim)\n",
    "opt = torch.optim.Adam(net.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1.\tNumber of swapped pairs: 34218/124750\tnDCG: 0.8830\n",
      "epoch: 1.\tNumber of swapped pairs: 31158/124750\tnDCG: 0.9030\n",
      "epoch: 1.\tNumber of swapped pairs: 28822/124750\tnDCG: 0.9139\n",
      "epoch: 1.\tNumber of swapped pairs: 26453/124750\tnDCG: 0.9240\n",
      "epoch: 1.\tNumber of swapped pairs: 24438/124750\tnDCG: 0.9333\n",
      "epoch: 1.\tNumber of swapped pairs: 22461/124750\tnDCG: 0.9406\n",
      "epoch: 1.\tNumber of swapped pairs: 20501/124750\tnDCG: 0.9499\n",
      "epoch: 2.\tNumber of swapped pairs: 20147/124750\tnDCG: 0.9513\n",
      "epoch: 2.\tNumber of swapped pairs: 18580/124750\tnDCG: 0.9567\n",
      "epoch: 2.\tNumber of swapped pairs: 16974/124750\tnDCG: 0.9627\n",
      "epoch: 2.\tNumber of swapped pairs: 15546/124750\tnDCG: 0.9674\n",
      "epoch: 2.\tNumber of swapped pairs: 14424/124750\tnDCG: 0.9712\n",
      "epoch: 2.\tNumber of swapped pairs: 13598/124750\tnDCG: 0.9737\n",
      "epoch: 2.\tNumber of swapped pairs: 12804/124750\tnDCG: 0.9760\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    idx = torch.randperm(N_train) #перемешиваем тренировочные данные и получаем новые индексы для тренировки \n",
    "\n",
    "    X_train = X_train[idx]\n",
    "    ys_train = ys_train[idx]\n",
    "\n",
    "    cur_batch = 0\n",
    "    for it in range(N_train // batch_size):\n",
    "        batch_X = X_train[cur_batch: cur_batch + batch_size]\n",
    "        batch_ys = ys_train[cur_batch: cur_batch + batch_size] #метки на указанный батч \n",
    "        cur_batch += batch_size\n",
    "\n",
    "        opt.zero_grad() #перед обучением зануляем градиенты \n",
    "        if len(batch_X) > 0: #проверка, что бантч не пустой \n",
    "            batch_pred = net(batch_X) #применяем модель и получаем логиты \n",
    "            batch_loss = listnet_kl_loss(batch_ys, batch_pred) #считаем кросс энтропию \n",
    "#             batch_loss = listnet_ce_loss(batch_ys, batch_pred)\n",
    "            batch_loss.backward(retain_graph=True) #считаем градиенты и применяем алогритм Backpropagation\n",
    "            opt.step() #шаг градиентного спуска \n",
    "\n",
    "        if it % 10 == 0: #каждый десятый батч делаем валидацию \n",
    "            with torch.no_grad():\n",
    "                valid_pred = net(X_valid) #скрамливаем всю валидационную группу модели и получаем предсказания \n",
    "                valid_swapped_pairs = num_swapped_pairs(ys_valid, valid_pred) #количество неправильно упорядоченных пар \n",
    "                ndcg_score = ndcg(ys_valid, valid_pred)\n",
    "            print(f\"epoch: {epoch + 1}.\\tNumber of swapped pairs: \" \n",
    "                  f\"{valid_swapped_pairs}/{N_valid * (N_valid - 1) // 2}\\t\"\n",
    "                  f\"nDCG: {ndcg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
